{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation para Display de 7 segmentos"
      ],
      "metadata": {
        "id": "XGi1NQrlJgVt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZAbkgEn_jor"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#funcao de ativacao (sigmoide)\n",
        "def sigmoid(x, derivative=False):\n",
        "    s = 1.0 / (1.0 + np.exp(-x))\n",
        "    #derivada da sigmoide\n",
        "    if derivative:\n",
        "        return s * (1 - s)\n",
        "    return s\n",
        "\n",
        "#funcao de ativacao (relu)\n",
        "def relu ( x, derivative= False ):\n",
        "  if derivative:\n",
        "      return  1 * (x > 0 )\n",
        "  return np.maximum( 0 , x)\n",
        "\n",
        "#funcao de ativacao (softmax)\n",
        "def softmax(x, derivative=False):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    probs = exps / np.sum(exps, axis=1, keepdims=True)\n",
        "    if derivative:\n",
        "        return probs\n",
        "    return probs\n",
        "\n",
        "#funcao de erro quadratico medio\n",
        "def mse(y_true, y_pred, derivative=False):\n",
        "    if derivative:\n",
        "        return (y_pred - y_true)\n",
        "    return np.mean(0.5 * (y_true - y_pred) ** 2)"
      ],
      "metadata": {
        "id": "qcfRu7tLEr4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-Network parameters\n",
        "m           = 0.9            #momentum\n",
        "a           = 0.01           #taxa de aprendizado\n",
        "init_method = 'xavier'       #tecnica para ajuste dos pesos\n",
        "hidden_f    = sigmoid        #funcao de ativacao para camada oculta\n",
        "output_f    = sigmoid        #funcao de ativacao camada de saida\n",
        "epochs      = 1000\n",
        "batch_size  = 0\n",
        "e_threshold = 1E-5           #criterio de parada antecipada\n",
        "\n",
        "network_topology = [7, 5, 10] # 7 neuronios de entrada, 5 na camada oculta, 10 na saida\n",
        "\n",
        "# Entradas do display (segmentos)\n",
        "display = np.array([\n",
        "    [1,1,1,1,1,1,0],  # 0\n",
        "    [0,1,1,0,0,0,0],  # 1\n",
        "    [1,1,0,1,1,0,1],  # 2\n",
        "    [1,1,1,1,0,0,1],  # 3\n",
        "    [0,1,1,0,0,1,1],  # 4\n",
        "    [1,0,1,1,0,1,1],  # 5\n",
        "    [1,0,1,1,1,1,1],  # 6\n",
        "    [1,1,1,0,0,0,0],  # 7\n",
        "    [1,1,1,1,1,1,1],  # 8\n",
        "    [1,1,1,1,0,1,1],  # 9\n",
        "])\n",
        "\n",
        "# Saídas esperadas (números em binário)\n",
        "saida_esperada = np.array([\n",
        "    [1,0,0,0,0,0,0,0,0,0],  # 0\n",
        "    [0,1,0,0,0,0,0,0,0,0],  # 1\n",
        "    [0,0,1,0,0,0,0,0,0,0],  # 2\n",
        "    [0,0,0,1,0,0,0,0,0,0],  # 3\n",
        "    [0,0,0,0,1,0,0,0,0,0],  # 4\n",
        "    [0,0,0,0,0,1,0,0,0,0],  # 5\n",
        "    [0,0,0,0,0,0,1,0,0,0],  # 6\n",
        "    [0,0,0,0,0,0,0,1,0,0],  # 7\n",
        "    [0,0,0,0,0,0,0,0,1,0],  # 8\n",
        "    [0,0,0,0,0,0,0,0,0,1],  # 9\n",
        "])"
      ],
      "metadata": {
        "id": "hCcWKdKhDL2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementação do Backpropagation\n",
        "class Backpropagation:\n",
        "\n",
        "    def __init__(self,\n",
        "                 topology,\n",
        "                 learning_rate = 0.01,\n",
        "                 momentum      = 0.9,\n",
        "                 hidden_activation_func=sigmoid,\n",
        "                 output_activation_func=sigmoid,\n",
        "                 init_method='xavier',\n",
        "                 seed=None):\n",
        "\n",
        "        self.topology = topology\n",
        "        self.learning_rate = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.hidden_activation = hidden_activation_func\n",
        "        self.output_activation = output_activation_func\n",
        "\n",
        "        self.error_function = mse\n",
        "\n",
        "        # Inicializações básicas\n",
        "        self.size = len(topology) - 1\n",
        "        self.weights = []\n",
        "        self.bias = []\n",
        "\n",
        "        for i in range(self.size):\n",
        "            in_neurons = topology[i]\n",
        "            out_neurons = topology[i + 1]\n",
        "            limit = np.sqrt(1 / in_neurons)\n",
        "            W = np.random.uniform(-limit, limit, (in_neurons, out_neurons))\n",
        "            b = np.zeros((1, out_neurons))\n",
        "            self.weights.append(W)\n",
        "            self.bias.append(b)\n",
        "\n",
        "\n",
        "    def feedforward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.netIns = []\n",
        "        self.deltas = [None] * self.size\n",
        "\n",
        "        layer_input = inputs # Inicializa com entradas da rede\n",
        "        self.netOuts = []\n",
        "\n",
        "        for i in range(self.size):\n",
        "            # Calcula o net input\n",
        "            netIn = np.dot(layer_input, self.weights[i]) + self.bias[i]\n",
        "            self.netIns.append(netIn)\n",
        "\n",
        "            # Aplica a função de ativação\n",
        "            if i == self.size - 1:\n",
        "                # Camada de saída\n",
        "                netOut = self.output_activation(netIn)\n",
        "            else:\n",
        "                # Camadas ocultas\n",
        "                netOut = self.hidden_activation(netIn)\n",
        "\n",
        "            self.netOuts.append(netOut)\n",
        "            layer_input = netOut\n",
        "\n",
        "        return self.netOuts[-1] # Retorna a saída da última camada\n",
        "\n",
        "\n",
        "    #calcular gradiente para camada de saida\n",
        "    def backprop(self, target, output, error_func):\n",
        "        # Percorre camadas de trás pra frente\n",
        "        for i in range(self.size):\n",
        "            back_index = self.size - 1 - i  # camada atual (de trás pra frente)\n",
        "\n",
        "            # --- CAMADA DE SAÍDA ---\n",
        "            if i == 0:\n",
        "                # Derivadas\n",
        "                d_activ = self.output_activation(self.netIns[back_index], derivative=True) #ativacao\n",
        "                d_error = error_func(target, output, derivative=True)  #erro\n",
        "                delta = d_error * d_activ  # erro local (gradiente)\n",
        "\n",
        "            # --- CAMADAS OCULTAS ---\n",
        "            else:\n",
        "                next_index = back_index + 1\n",
        "                W_next = self.weights[next_index]\n",
        "                delta_next = self.deltas[next_index]\n",
        "                d_activ = self.hidden_activation(self.netIns[back_index], derivative=True)\n",
        "\n",
        "                # Propagação do erro\n",
        "                delta = np.dot(delta_next, W_next.T) * d_activ\n",
        "\n",
        "            # Guarda o delta atual\n",
        "            self.deltas[back_index] = delta\n",
        "\n",
        "            # --- GRADIENTES ---\n",
        "            if back_index == 0:\n",
        "                # entrada original da rede\n",
        "                layer_input = self.inputs\n",
        "            else:\n",
        "                layer_input = self.netOuts[back_index - 1]\n",
        "\n",
        "            gradient_mat = np.dot(layer_input.T, delta)\n",
        "            bias_grad_mat = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "            # --- ATUALIZA PESOS ---\n",
        "            self._gradient_descent(\n",
        "                layer_idx=back_index,\n",
        "                gradient_mat=gradient_mat,\n",
        "                bias_gradient=bias_grad_mat\n",
        "            )\n",
        "\n",
        "\n",
        "    def _gradient_descent(self, layer_idx, gradient_mat, bias_gradient):\n",
        "        self.weights[layer_idx] -= self.learning_rate * gradient_mat\n",
        "        self.bias[layer_idx] -= self.learning_rate * bias_gradient\n",
        "\n",
        "    def train(self, inputs, targets, epochs=10000, error_threshold=1e-3):\n",
        "        for epoch in range(epochs):\n",
        "            # Feedforward\n",
        "            outputs = self.feedforward(inputs)\n",
        "\n",
        "            # Calcula erro\n",
        "            error = self.error_function(targets, outputs)\n",
        "\n",
        "            # Backpropagation\n",
        "            self.backprop(targets, outputs, self.error_function)\n",
        "\n",
        "            if epoch % 1000 == 0:\n",
        "                print(f\"Época {epoch} - Erro: {error:.6f}\")\n",
        "            if error <= error_threshold:\n",
        "                break"
      ],
      "metadata": {
        "id": "d9CStdzS_n9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nnet = Backpropagation(\n",
        "    topology=network_topology,\n",
        "    hidden_activation_func=hidden_f,\n",
        "    output_activation_func=output_f,\n",
        "    init_method=init_method,\n",
        "    momentum=m,\n",
        "    learning_rate=a\n",
        ")\n",
        "\n",
        "error = nnet.train(\n",
        "    display,\n",
        "    saida_esperada,\n",
        "    epochs=epochs,\n",
        "    error_threshold=e_threshold\n",
        ")\n",
        "\n",
        "print(f\"\\n\\n{'='*40}\\nTraining Display 7 Segmentos:\\n{'='*40}\\n\")\n",
        "\n",
        "for i, sample in enumerate(display):\n",
        "    output = nnet.feedforward(sample.reshape(1, -1))\n",
        "    sample_error = mse(saida_esperada[i:i+1], output)\n",
        "    print(f\"Testing Network:\\n\\tvetor de entrada : {sample}\\n\\tvetor de saída   : {output}\\n\\tsaída esperada   : {saida_esperada[i]}\")\n",
        "    print(f\"\\tNetwork error   : {sample_error:.3e}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flqhh1s7EXej",
        "outputId": "c204d1f2-9cf4-4aa3-ebe7-78620950d2cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 0 - Erro: 0.115469\n",
            "\n",
            "\n",
            "========================================\n",
            "Training Display 7 Segmentos:\n",
            "========================================\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 1 1 1 1 1 0]\n",
            "\tvetor de saída   : [[0.10115417 0.11024169 0.10527132 0.10012655 0.10185235 0.09649601\n",
            "  0.10176674 0.10254862 0.09396168 0.09615603]]\n",
            "\tsaída esperada   : [1 0 0 0 0 0 0 0 0 0]\n",
            "\tNetwork error   : 4.499e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [0 1 1 0 0 0 0]\n",
            "\tvetor de saída   : [[0.12936969 0.14385287 0.11974921 0.11668467 0.13013149 0.11788968\n",
            "  0.11505697 0.12798516 0.11391721 0.11467814]]\n",
            "\tsaída esperada   : [0 1 0 0 0 0 0 0 0 0]\n",
            "\tNetwork error   : 4.321e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 1 0 1 1 0 1]\n",
            "\tvetor de saída   : [[0.09896585 0.1097547  0.11592469 0.09839992 0.10275146 0.10575156\n",
            "  0.10792508 0.10670987 0.10113091 0.10172884]]\n",
            "\tsaída esperada   : [0 0 1 0 0 0 0 0 0 0]\n",
            "\tNetwork error   : 4.392e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 1 1 1 0 0 1]\n",
            "\tvetor de saída   : [[0.09554758 0.10572832 0.10265417 0.09948844 0.09744279 0.0948044\n",
            "  0.10019846 0.09996388 0.09552957 0.09786861]]\n",
            "\tsaída esperada   : [0 0 0 1 0 0 0 0 0 0]\n",
            "\tNetwork error   : 4.495e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [0 1 1 0 0 1 1]\n",
            "\tvetor de saída   : [[0.10542926 0.11669513 0.10274711 0.10286039 0.10684171 0.09916395\n",
            "  0.10150068 0.10892012 0.09870813 0.10052259]]\n",
            "\tsaída esperada   : [0 0 0 0 1 0 0 0 0 0]\n",
            "\tNetwork error   : 4.477e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 0 1 1 0 1 1]\n",
            "\tvetor de saída   : [[0.09648827 0.10445783 0.10647163 0.10469363 0.0984951  0.09734509\n",
            "  0.10264334 0.09938029 0.09893444 0.10218673]]\n",
            "\tsaída esperada   : [0 0 0 0 0 1 0 0 0 0]\n",
            "\tNetwork error   : 4.538e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 0 1 1 1 1 1]\n",
            "\tvetor de saída   : [[0.09224201 0.10123306 0.10781438 0.09848196 0.09496292 0.09677911\n",
            "  0.10253182 0.09772608 0.09627443 0.09852522]]\n",
            "\tsaída esperada   : [0 0 0 0 0 0 1 0 0 0]\n",
            "\tNetwork error   : 4.462e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 1 1 0 0 0 0]\n",
            "\tvetor de saída   : [[0.11533392 0.12900132 0.11400447 0.10956581 0.11654239 0.10876991\n",
            "  0.1099301  0.11639797 0.10674121 0.10801146]]\n",
            "\tsaída esperada   : [0 0 0 0 0 0 0 1 0 0]\n",
            "\tNetwork error   : 4.481e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 1 1 1 1 1 1]\n",
            "\tvetor de saída   : [[0.08970318 0.0990468  0.09851147 0.09258043 0.09158117 0.08976377\n",
            "  0.09626993 0.09541372 0.08899845 0.09102721]]\n",
            "\tsaída esperada   : [0 0 0 0 0 0 0 0 1 0]\n",
            "\tNetwork error   : 4.546e-02\n",
            "\n",
            "Testing Network:\n",
            "\tvetor de entrada : [1 1 1 1 0 1 1]\n",
            "\tvetor de saída   : [[0.09333766 0.10205248 0.09745473 0.09802577 0.09457605 0.09023119\n",
            "  0.09647788 0.09679275 0.09146719 0.09434307]]\n",
            "\tsaída esperada   : [0 0 0 0 0 0 0 0 0 1]\n",
            "\tNetwork error   : 4.513e-02\n",
            "\n"
          ]
        }
      ]
    }
  ]
}